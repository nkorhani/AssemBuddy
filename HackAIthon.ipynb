{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfccdaf1-0c10-4765-9a64-acbe03723c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip install llama_parse langchain langchain-community langchain-groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a665e8dc-7f65-4007-859e-3fc99498b96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "import os\n",
    "load_dotenv('local.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619b4a3b-ab14-4200-82f1-c979007937e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llamaparse_api_key = os.getenv('LLAMA_API_KEY')\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6c34e2-41c7-4efb-b554-e822ba2f0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LLAMAPARSE #####\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "#\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "#\n",
    "import joblib\n",
    "import os\n",
    "import nest_asyncio  # noqa: E402\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fde4f3b-9c8b-457d-95d9-31faa006cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "#\n",
    "def load_or_parse_data(pdf_path):\n",
    "    data_file = \"./data/parsed_data.pkl\"\n",
    "\n",
    "    if os.path.exists(data_file):\n",
    "        # Load the parsed data from the file\n",
    "        parsed_data = joblib.load(data_file)\n",
    "    else:\n",
    "        # Perform the parsing step and store the result in llama_parse_documents\n",
    "        parsingInstructionUber10k = \"\"\"The provided document is an installation manual for a \n",
    "        specific product.\n",
    "        Try to be precise while answering the questions\"\"\"\n",
    "        parser = LlamaParse(api_key=llamaparse_api_key,\n",
    "                            result_type=\"markdown\",\n",
    "                            parsing_instruction=parsingInstructionUber10k,\n",
    "                            max_timeout=5000,)\n",
    "        llama_parse_documents = parser.load_data(pdf_path)\n",
    "\n",
    "\n",
    "        # Save the parsed data to a file\n",
    "        print(\"Saving the parse results in .pkl format ..........\")\n",
    "        joblib.dump(llama_parse_documents, data_file)\n",
    "\n",
    "        # Set the parsed data to the variable\n",
    "        parsed_data = llama_parse_documents\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435cdad8-2c87-4006-9d50-96868ff4d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector database\n",
    "def create_vector_database(pdf_path):\n",
    "    \"\"\"\n",
    "    Creates a vector database using document loaders and embeddings.\n",
    "\n",
    "    This function loads urls,\n",
    "    splits the loaded documents into chunks, transforms them into embeddings using OllamaEmbeddings,\n",
    "    and finally persists the embeddings into a Chroma vector database.\n",
    "\n",
    "    \"\"\"\n",
    "    # Call the function to either load or parse the data\n",
    "    llama_parse_documents = load_or_parse_data(pdf_path)\n",
    "    print(llama_parse_documents[0].text[:300])\n",
    "\n",
    "    with open('data/output.md', 'a') as f:  # Open the file in append mode ('a')\n",
    "        for doc in llama_parse_documents:\n",
    "            f.write(doc.text + '\\n')\n",
    "\n",
    "    markdown_path = \"data/output.md\"\n",
    "    loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "   #loader = DirectoryLoader('data/', glob=\"**/*.md\", show_progress=True)\n",
    "    documents = loader.load()\n",
    "    # Split loaded documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    #len(docs)\n",
    "    print(f\"length of documents loaded: {len(documents)}\")\n",
    "    print(f\"total number of document chunks generated :{len(docs)}\")\n",
    "    #docs[0]\n",
    "\n",
    "    # Initialize Embeddings\n",
    "    embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "    # Create and persist a Chroma vector database from the chunked documents\n",
    "    vs = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embed_model,\n",
    "        persist_directory=\"chroma_db_llamaparse1\",  # Local mode with in-memory storage only\n",
    "        collection_name=\"rag\"\n",
    "    )\n",
    "\n",
    "    #query it\n",
    "    #query = \"what is the agend of Financial Statements for 2022 ?\"\n",
    "    #found_doc = qdrant.similarity_search(query, k=3)\n",
    "    #print(found_doc[0][:100])\n",
    "    #print(qdrant.get())\n",
    "\n",
    "    print('Vector DB created successfully !')\n",
    "    return vs,embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a83da6b1-6de9-4116-8ab9-29b18714f814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "\n",
      "# T6 Pro Programmable Thermostat Installation Instructions\n",
      "\n",
      "# T6 Pro Programmable Thermostat Installation Instructions\n",
      "\n",
      "# Package Includes:\n",
      "\n",
      "- T6 Pro Thermostat\n",
      "- UWP™ Mounting System\n",
      "- Honeywell Standard Installation Adapter (J-box adapter)\n",
      "- Honeywell Decorative Cover Plate – Small; size 4-49/6\n",
      "length of documents loaded: 1\n",
      "total number of document chunks generated :90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b58bb29d184e189c77c10a3c74f8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB created successfully !\n"
     ]
    }
   ],
   "source": [
    "PDF_PATH = \"/Users/navidkorhani/Downloads/Honeywell_T60Pro.pdf\"\n",
    "vs, embed_model = create_vector_database(PDF_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86123e82-2a99-4f22-b8fc-d14da3fbe01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatGroq(temperature=0,\n",
    "                      model_name=\"mixtral-8x7b-32768\",\n",
    "                      api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8784423-7d09-4a35-9b9a-f3ebf9d143af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(embedding_function=embed_model,\n",
    "                  persist_directory=\"chroma_db_llamaparse1\",\n",
    "                  collection_name=\"rag\")\n",
    "#\n",
    "retriever=vectorstore.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16fb81c9-d4e3-463c-a4d2-1c310f893d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebb50a53-67cd-4ded-8743-e15f35f00f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of information to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "#\n",
    "prompt = set_custom_prompt()\n",
    "prompt\n",
    "\n",
    "########################### RESPONSE ###########################\n",
    "PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of information to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22935fc0-6c6a-4f31-ba8e-df7094ac2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=chat_model,\n",
    "                               chain_type=\"stuff\",\n",
    "                               retriever=retriever,\n",
    "                               return_source_documents=True,\n",
    "                               chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6882e289-2456-4be4-aad8-2e2e4fb12151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "response = qa.invoke({\"query\": \"Help me Troubleshoot why the Display is blank?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c52d47c-3771-40de-9716-d6cc5ddd8b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If the display on your device is blank, you can try the following steps to troubleshoot the issue:\\n\\n1. Check the circuit breaker and reset if necessary.\\n2. Make sure the power switch for the heating and cooling system is turned on.\\n3. Check that the furnace door is closed securely.\\n4. Make sure that fresh AA alkaline batteries are correctly installed.\\n\\nIf you have tried these steps and the display is still blank, you may want to consider contacting the manufacturer or a professional for further assistance.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "216dc4aa-3818-4f0e-bcec-c972fd1e79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "load_dotenv(\"keys.env\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Adaptive RAG\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c9ff32b-dcf2-477d-926e-a61f9fa84d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63e70fb3-87af-4de2-b449-53e905141bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9718161-8af7-4862-b1f0-d04491560458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "print(\n",
    "    question_router.invoke(\n",
    "        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n",
    "    )\n",
    ")\n",
    "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ce3ff-ba46-4c6a-8b17-23580b7a0c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
